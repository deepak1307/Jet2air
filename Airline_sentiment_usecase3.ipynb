{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepak bhaiya usecase3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAPCGp9qnaUr",
        "outputId": "93efbbb0-ac9f-474a-8e9f-0195bdaefb8c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr  4 06:44:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WAIWLLInysO",
        "outputId": "bc100dc8-87c3-471c-c37c-9fc3f4abb05f"
      },
      "source": [
        "# install the required bert pretrained model\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 24.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 17.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 17.2MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 26.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 35.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 36.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 39.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 41.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 34.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 36.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/10/a997a266165e2df1976c4fc973f71bcd2e65a255f92d0ff7ab59b2f81989/boto3-1.17.44-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.44\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/80/3ddbe4ad2561804b887deb8072d802dc24dd759833139a5b91efcff308d6/botocore-1.20.44-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.44->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.44->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.44 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed boto3-1.17.44 botocore-1.20.44 jmespath-0.10.0 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ZJmOCZn23m"
      },
      "source": [
        "# import the required libraries.\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5ooCGu7kn52K",
        "outputId": "f1ced5f6-d7cb-4fed-fa05-3bf5e8e32b4c"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIEauGh9n82h"
      },
      "source": [
        "df = pd.read_csv('/content/Usecase3_Dataset.csv',encoding='utf-8')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jw2P-1kZoBeo",
        "outputId": "b85824ec-5f85-4e4d-b57c-1f8cb95e8c4a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment  ...                                               text\n",
              "0           neutral  ...                @VirginAmerica What @dhepburn said.\n",
              "1          positive  ...  @VirginAmerica plus you've added commercials t...\n",
              "2           neutral  ...  @VirginAmerica I didn't today... Must mean I n...\n",
              "3          negative  ...  @VirginAmerica it's really aggressive to blast...\n",
              "4          negative  ...  @VirginAmerica and it's a really big bad thing...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "FEYccjlooC6x",
        "outputId": "301bf9b2-c7ae-40b7-ebe7-5c2bd9e0b83e"
      },
      "source": [
        "df['airline_sentiment'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa15047910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "TjFgLdF4oHKx",
        "outputId": "d7e45f91-c933-4709-e170-acba6f3cbeb8"
      },
      "source": [
        "df['airline'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa1506de90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "99Qor7_-oI-5",
        "outputId": "03365df8-f7fd-4aea-d7ea-632392a09fe3"
      },
      "source": [
        "df.rename(columns={'airline_sentiment':'label'},inplace=True)\n",
        "df = df[['label','text']]\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text\n",
              "0   neutral                @VirginAmerica What @dhepburn said.\n",
              "1  positive  @VirginAmerica plus you've added commercials t...\n",
              "2   neutral  @VirginAmerica I didn't today... Must mean I n...\n",
              "3  negative  @VirginAmerica it's really aggressive to blast...\n",
              "4  negative  @VirginAmerica and it's a really big bad thing..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "keptzGiGoNUo",
        "outputId": "c61aeeb1-5d68-4890-c22e-7dfbc0d55913"
      },
      "source": [
        "df.loc[:,'sentiment'] = df.label.map({'negative':0,'neutral':2,'positive':1})\n",
        "df = df.drop(['label'], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                @VirginAmerica What @dhepburn said.          2\n",
              "1  @VirginAmerica plus you've added commercials t...          1\n",
              "2  @VirginAmerica I didn't today... Must mean I n...          2\n",
              "3  @VirginAmerica it's really aggressive to blast...          0\n",
              "4  @VirginAmerica and it's a really big bad thing...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9m_WDpCoP89"
      },
      "source": [
        "def preprocess_tweet(tweet):\n",
        "\t#Preprocess the text in a single tweet\n",
        "\t#arguments: tweet = a single tweet in form of string \n",
        "\t#convert the tweet to lower case\n",
        "\ttweet.lower()\n",
        "\t#convert all urls to sting \"URL\"\n",
        "\ttweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
        "\t#convert all @username to \"at_user\"\n",
        "  #tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
        "\t#correct all multiple white spaces to a single white space\n",
        "\ttweet = re.sub('[\\s]+', ' ', tweet)\n",
        "\t#convert \"#topic\" to just \"topic\"\n",
        "\ttweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "\treturn tweet"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hEkTlB9WoScl",
        "outputId": "8fa5dc99-e3a0-485a-8569-339ee39196b2"
      },
      "source": [
        "df['text'] = df['text'].apply(preprocess_tweet)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                @VirginAmerica What @dhepburn said.          2\n",
              "1  @VirginAmerica plus you've added commercials t...          1\n",
              "2  @VirginAmerica I didn't today... Must mean I n...          2\n",
              "3  @VirginAmerica it's really aggressive to blast...          0\n",
              "4  @VirginAmerica and it's a really big bad thing...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QPbbkkBoVSc"
      },
      "source": [
        "sentences = df.text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrUOvvgpoZ6B",
        "outputId": "0465f336-8704-4170-b07e-0a9a86a2dda3"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 16644353.36B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', '@', 'virgin', '##ame', '##rica', 'what', '@', 'dh', '##ep', '##burn', 'said', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBM32vW0obwg",
        "outputId": "240d302a-a851-45c3-e104-30453268ef0f"
      },
      "source": [
        "#loading the labels value\n",
        "labels = df.sentiment.values\n",
        "#finding maximum length of a tweet\n",
        "print(df.text.map(lambda x: len(x)).max())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBpLDUqwodaP"
      },
      "source": [
        "# Set the maximum sequence length.\n",
        "#The longest sequence in our training set its 186\n",
        "# We will set it to 256\n",
        "MAX_LEN = 256"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uovsso3uojSV"
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkLA2iLPolHn"
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LQ7rTz6onYi"
      },
      "source": [
        "#Pad the sequences\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnEGS4TTopiE",
        "outputId": "b8ee07a3-1b61-4b6a-b39c-7d72f08f6ddd"
      },
      "source": [
        "#Printing to see how the second tweet after preprocessing looks.\n",
        "input_ids[1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  101,  1030,  6261, 14074, 14735,  4606,  2017,  1005,  2310,\n",
              "        2794, 12698,  2000,  1996,  3325,  1012,  1012,  1012, 26997,\n",
              "        2100,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z20EsmhosaL"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtaT4Fi9ovkm"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCBZWMzHoxsw"
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aItRbacBo0O4"
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task\n",
        "# A batch size of 16 or 32 is preferred \n",
        "# If Cuda goes out of memory try lowering the batch_size.\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjuAEcrYo2ka",
        "outputId": "a3bca7ad-8ef7-4022-d482-e1e200268f4c"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "# Make the num_label = 3 positive,negative,neutral\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "model.cuda()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:06<00:00, 60385417.22B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGv9Aws9o448"
      },
      "source": [
        "# Set different weight decays for different layers of the model.\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsLRiF5apBCl",
        "outputId": "268fdf8e-ddde-44ed-a5b4-ff4549dd734c"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=3e-5,\n",
        "                     warmup=.1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2fOELGtpDQa"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INtnFAwvpGGt",
        "outputId": "bd4e0c7c-b740-4ae9-9e4a-f55fd5565215"
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 10\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FUCWimopIy6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}